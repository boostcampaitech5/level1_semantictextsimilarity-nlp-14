{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'py-hanspell'...\n",
      "remote: Enumerating objects: 101, done.\u001b[K\n",
      "remote: Counting objects: 100% (20/20), done.\u001b[K\n",
      "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
      "remote: Total 101 (delta 5), reused 10 (delta 3), pack-reused 81\u001b[K\n",
      "Receiving objects: 100% (101/101), 25.27 KiB | 12.64 MiB/s, done.\n",
      "Resolving deltas: 100% (42/42), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ssut/py-hanspell.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/ml/workspace/level1_semantictextsimilarity-nlp-14/JS/py-hanspell\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "running install\n",
      "running bdist_egg\n",
      "running egg_info\n",
      "writing py_hanspell.egg-info/PKG-INFO\n",
      "writing dependency_links to py_hanspell.egg-info/dependency_links.txt\n",
      "writing requirements to py_hanspell.egg-info/requires.txt\n",
      "writing top-level names to py_hanspell.egg-info/top_level.txt\n",
      "reading manifest file 'py_hanspell.egg-info/SOURCES.txt'\n",
      "writing manifest file 'py_hanspell.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.linux-x86_64/egg\n",
      "running install_lib\n",
      "running build_py\n",
      "copying hanspell/spell_checker.py -> build/lib/hanspell\n",
      "copying hanspell/constants.py -> build/lib/hanspell\n",
      "creating build/bdist.linux-x86_64/egg\n",
      "creating build/bdist.linux-x86_64/egg/hanspell\n",
      "copying build/lib/hanspell/__init__.py -> build/bdist.linux-x86_64/egg/hanspell\n",
      "copying build/lib/hanspell/spell_checker.py -> build/bdist.linux-x86_64/egg/hanspell\n",
      "copying build/lib/hanspell/constants.py -> build/bdist.linux-x86_64/egg/hanspell\n",
      "copying build/lib/hanspell/response.py -> build/bdist.linux-x86_64/egg/hanspell\n",
      "byte-compiling build/bdist.linux-x86_64/egg/hanspell/__init__.py to __init__.cpython-38.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/hanspell/spell_checker.py to spell_checker.cpython-38.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/hanspell/constants.py to constants.cpython-38.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/hanspell/response.py to response.cpython-38.pyc\n",
      "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying py_hanspell.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying py_hanspell.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying py_hanspell.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying py_hanspell.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying py_hanspell.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "zip_safe flag not set; analyzing archive contents...\n",
      "creating 'dist/py_hanspell-1.1-py3.8.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
      "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
      "Processing py_hanspell-1.1-py3.8.egg\n",
      "Removing /opt/conda/lib/python3.8/site-packages/py_hanspell-1.1-py3.8.egg\n",
      "Copying py_hanspell-1.1-py3.8.egg to /opt/conda/lib/python3.8/site-packages\n",
      "py-hanspell 1.1 is already the active version in easy-install.pth\n",
      "\n",
      "Installed /opt/conda/lib/python3.8/site-packages/py_hanspell-1.1-py3.8.egg\n",
      "Processing dependencies for py-hanspell==1.1\n",
      "Searching for requests==2.28.2\n",
      "Best match: requests 2.28.2\n",
      "Adding requests 2.28.2 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.8/site-packages\n",
      "Searching for charset-normalizer==3.1.0\n",
      "Best match: charset-normalizer 3.1.0\n",
      "Adding charset-normalizer 3.1.0 to easy-install.pth file\n",
      "Installing normalizer script to /opt/conda/bin\n",
      "\n",
      "Using /opt/conda/lib/python3.8/site-packages\n",
      "Searching for urllib3==1.26.15\n",
      "Best match: urllib3 1.26.15\n",
      "Adding urllib3 1.26.15 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.8/site-packages\n",
      "Searching for idna==2.10\n",
      "Best match: idna 2.10\n",
      "Adding idna 2.10 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.8/site-packages\n",
      "Searching for certifi==2022.12.7\n",
      "Best match: certifi 2022.12.7\n",
      "Adding certifi 2022.12.7 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.8/site-packages\n",
      "Finished processing dependencies for py-hanspell==1.1\n",
      "/opt/ml/workspace/level1_semantictextsimilarity-nlp-14/JS\n"
     ]
    }
   ],
   "source": [
    "%cd ./py-hanspell\n",
    "!python ./setup.py install\n",
    "\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[0m\u001b[01;34m09103base\u001b[0m/                \u001b[01;34mdata_test\u001b[0m/              output09105.csv\n",
      "\u001b[01;34m2023-04-12-002base_sota\u001b[0m/  \u001b[01;34mklue-roberta-small_JS\u001b[0m/  output09147.csv\n",
      "\u001b[01;34m2023-04-16-001base\u001b[0m/       \u001b[01;34mlarge_long\u001b[0m/             output_V3.csv\n",
      "Readme.md                 \u001b[01;34mlightning_logs\u001b[0m/         \u001b[01;34mpy-hanspell\u001b[0m/\n",
      "\u001b[01;34mSTS_roberta_large\u001b[0m/        \u001b[01;34mload_09103\u001b[0m/             \u001b[01;34msave\u001b[0m/\n",
      "\u001b[01;34mSTS_roberta_large_002\u001b[0m/    model.pt                spell_correction.ipynb\n",
      "\u001b[01;34mcode\u001b[0m/                     nohup.out               \u001b[01;34mwandb\u001b[0m/\n",
      "data.ipynb                output09103.csv\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9324it [00:22, 414.13it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>sentence_1</th>\n",
       "      <th>sentence_2</th>\n",
       "      <th>label</th>\n",
       "      <th>binary-label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>boostcamp-sts-v1-train-000</td>\n",
       "      <td>nsmc-sampled</td>\n",
       "      <td>스릴도있고 반전도 있고 여느 한국영화 쓰레기들하고는 차원이 다르네요~</td>\n",
       "      <td>반전도 있고,사랑도 있고재미도있네요.</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>boostcamp-sts-v1-train-001</td>\n",
       "      <td>slack-rtt</td>\n",
       "      <td>앗 제가 접근 권한이 없다고 뜹니다;;</td>\n",
       "      <td>오, 액세스 권한이 없다고 합니다.</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>boostcamp-sts-v1-train-002</td>\n",
       "      <td>petition-sampled</td>\n",
       "      <td>주택청약조건 변경해주세요.</td>\n",
       "      <td>주택청약 무주택기준 변경해주세요.</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boostcamp-sts-v1-train-003</td>\n",
       "      <td>slack-sampled</td>\n",
       "      <td>입사후 처음 대면으로 만나 반가웠습니다.</td>\n",
       "      <td>화상으로만 보다가 리얼로 만나니 정말 반가웠습니다.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>boostcamp-sts-v1-train-004</td>\n",
       "      <td>slack-sampled</td>\n",
       "      <td>뿌듯뿌듯 하네요!!</td>\n",
       "      <td>꼬옥 실제로 한번 뵈어요 뿌뿌뿌~!~!</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           id            source  \\\n",
       "0  boostcamp-sts-v1-train-000      nsmc-sampled   \n",
       "1  boostcamp-sts-v1-train-001         slack-rtt   \n",
       "2  boostcamp-sts-v1-train-002  petition-sampled   \n",
       "3  boostcamp-sts-v1-train-003     slack-sampled   \n",
       "4  boostcamp-sts-v1-train-004     slack-sampled   \n",
       "\n",
       "                               sentence_1                    sentence_2  \\\n",
       "0  스릴도있고 반전도 있고 여느 한국영화 쓰레기들하고는 차원이 다르네요~          반전도 있고,사랑도 있고재미도있네요.   \n",
       "1                   앗 제가 접근 권한이 없다고 뜹니다;;           오, 액세스 권한이 없다고 합니다.   \n",
       "2                          주택청약조건 변경해주세요.            주택청약 무주택기준 변경해주세요.   \n",
       "3                  입사후 처음 대면으로 만나 반가웠습니다.  화상으로만 보다가 리얼로 만나니 정말 반가웠습니다.   \n",
       "4                              뿌듯뿌듯 하네요!!         꼬옥 실제로 한번 뵈어요 뿌뿌뿌~!~!   \n",
       "\n",
       "   label  binary-label  \n",
       "0    2.2           0.0  \n",
       "1    4.2           1.0  \n",
       "2    2.4           0.0  \n",
       "3    3.0           1.0  \n",
       "4    0.0           0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 기존 데이터세트에서 UNK로 인식되는 경우에는 스펠링 체크한 것으로 변경해주는 것이 우선이다(0417)\n",
    "# 기존 데이터세트 UNK, 스펠링 체크진행\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "sys.path.append('./py-hanspell')\n",
    "from tqdm import tqdm\n",
    "from hanspell import spell_checker\n",
    "\n",
    "\n",
    "train_path = '~/data/train.csv'\n",
    "dev_path = '~/data/dev.csv'\n",
    "\n",
    "train_data = pd.read_csv(train_path)\n",
    "dev_data = pd.read_csv(dev_path)\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('klue/roberta-small')\n",
    "\n",
    "train_data = pd.read_csv(train_path)\n",
    "train_data_unk_corrected = train_data.copy(deep=True)\n",
    "\n",
    "for i, item in tqdm(train_data.iterrows()):\n",
    "    sentence_1, sentence_2 = item['sentence_1'], item['sentence_2']\n",
    "    unk1_tokens, unk2_tokens = [], []\n",
    "    \n",
    "    # 1-1. UNK check - sentene 1\n",
    "    encode_1, encode_2 = tokenizer(sentence_1), tokenizer(sentence_2)\n",
    "    if encode_1['input_ids'].count(tokenizer.unk_token_id):\n",
    "        unk_tok_1 = []\n",
    "        \n",
    "        decode_1 = tokenizer.convert_ids_to_tokens(encode_1['input_ids'])\n",
    "        unk_idx = [i for i, token in enumerate(decode_1) if token == tokenizer.unk_token]\n",
    "        \n",
    "        for _idx in unk_idx:\n",
    "            char_index = encode_1.token_to_chars(_idx)\n",
    "            original_token = sentence_1[char_index.start:char_index.end]  # char_index 는 CharSpan(start=15, end=19) 형태로 리턴되더랍니다... 신기!\n",
    "            \n",
    "            unk_tok_1.append(original_token)\n",
    "        \n",
    "        if unk_tok_1:\n",
    "            unk1_tokens.append(unk_tok_1)\n",
    "            \n",
    "        # 2. spelling correction\n",
    "        result_1 = spell_checker.check(sentence_1).as_dict()\n",
    "        checked_1 = result_1['checked']\n",
    "        \n",
    "        if checked_1 != sentence_1:      # 하나라도 스펠링 바뀌면 바꾼 것으로 대체해서 넣는다.\n",
    "            train_data_unk_corrected.loc[i, 'sentence_1'] = checked_1\n",
    "        \n",
    "    \n",
    "    # 1-2. UNK check - sentene 2\n",
    "    if encode_2['input_ids'].count(tokenizer.unk_token_id):\n",
    "        unk_tok_2 = []\n",
    "        \n",
    "        decode_2 = tokenizer.convert_ids_to_tokens(encode_2['input_ids'])\n",
    "        unk_idx = [i for i, token in enumerate(decode_2) if token == tokenizer.unk_token]\n",
    "        \n",
    "        for _idx in unk_idx:\n",
    "            char_index = encode_2.token_to_chars(_idx)\n",
    "            original_token = sentence_2[char_index.start:char_index.end]  # char_index 는 CharSpan(start=15, end=19) 형태로 리턴되더랍니다... 신기!\n",
    "            \n",
    "            unk_tok_2.append(original_token)\n",
    "        \n",
    "        if unk_tok_2:\n",
    "            unk2_tokens.append(unk_tok_2)\n",
    "            \n",
    "        # 2. spelling correction\n",
    "        result_2 = spell_checker.check(sentence_2).as_dict()\n",
    "        checked_2 = result_2['checked']\n",
    "        \n",
    "        if checked_2 != sentence_2:      # 하나라도 스펠링 바뀌면 바꾼 것으로 대체해서 넣는다.\n",
    "            train_data_unk_corrected.loc[i, 'sentence_2'] = checked_2\n",
    "    \n",
    "train_data_unk_corrected.to_csv(\"~/data/train_unk_corrected.csv\")\n",
    "train_data_unk_corrected.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
