{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNK token을 포함한 문장에는 무엇이 있는가 확인하는 ipynb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 기초 함수 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unk sentence to csv\n",
    "\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "train_path = './data/train.csv'\n",
    "dev_path = './data/dev.csv'\n",
    "\n",
    "train_data = pd.read_csv(train_path)\n",
    "dev_data = pd.read_csv(dev_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('klue/roberta-small')\n",
    "\n",
    "# Function Settings\n",
    "def get_num_tokens(df):\n",
    "    sentence1_len, sentence2_len = [], []\n",
    "    sentence1_unk, sentence2_unk = [], []\n",
    "    \n",
    "    for i, item in df.iterrows():\n",
    "        sentence1 = tokenizer(item['sentence_1'])['input_ids']\n",
    "        sentence2 = tokenizer(item['sentence_2'])['input_ids']\n",
    "\n",
    "        sentence1_len.append(len(sentence1))\n",
    "        sentence2_len.append(len(sentence2))\n",
    "\n",
    "        sentence1_unk.append(sentence1.count(tokenizer.unk_token_id))\n",
    "        sentence2_unk.append(sentence2.count(tokenizer.unk_token_id))\n",
    "\n",
    "    return sentence1_len, sentence2_len, sentence1_unk, sentence2_unk\n",
    "    # return pd.DataFrame({'number of tokens':sentence1_len, 'label score':df.label.values.tolist()})\n",
    "\n",
    "\n",
    "# 1. 전체 df에 대해 score 5단계로 분류, 열 추가\n",
    "train_data_scored = train_data.copy(deep=True)\n",
    "score_integer = []\n",
    "\n",
    "for i, item in train_data_scored.iterrows():\n",
    "    label_value = int(item['label'])\n",
    "    if   label_value == 0:  col = 0\n",
    "    elif label_value < 2.0: col = 1\n",
    "    elif label_value < 3.0: col = 2\n",
    "    elif label_value < 4.0: col = 3\n",
    "    elif label_value < 5.0: col = 4\n",
    "    else:                   col = 5\n",
    "        \n",
    "    score_integer.append(col)\n",
    "train_data_scored['score_class'] = score_integer\n",
    "\n",
    "# 2. sentence 별 토큰 개수 넣기\n",
    "s1_len, s2_len, s1_unk, s2_unk = get_num_tokens(train_data_scored)\n",
    "\n",
    "train_data_scored['s1_num_tokens'] = s1_len\n",
    "train_data_scored['s2_num_tokens'] = s2_len\n",
    "train_data_scored['s1_num_unk'] = s1_unk\n",
    "train_data_scored['s2_num_unk'] = s2_unk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. unk token 문장 확인 후 CSV 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UNK> 토큰을 포함하고 있는 문장의 전체 개수는 453 개 입니다.\n",
      "해당 문장들을 csv파일로 저장하였습니다. 파일 이름: unk_setences_train.csv\n"
     ]
    }
   ],
   "source": [
    "# unk sentence to csv\n",
    "\n",
    "unk1_token_sentence = []\n",
    "unk2_token_sentence = []\n",
    "unk1_count, unk2_count = [], []\n",
    "\n",
    "source_list = sorted(train_data['source'].unique())\n",
    "\n",
    "for source_name in source_list:\n",
    "    g = train_data_scored.groupby(['source']).get_group(source_name)\n",
    "    u1_sentence = g[g['s1_num_unk'] >=1]['sentence_1'].values.tolist()\n",
    "    u2_sentence = g[g['s2_num_unk'] >=1]['sentence_2'].values.tolist()\n",
    "\n",
    "    unk1_token_sentence.extend(u1_sentence)\n",
    "    unk2_token_sentence.extend(u2_sentence)\n",
    "    unk1_count.extend(g[g['s1_num_unk'] >=1]['s1_num_unk'].values.tolist())\n",
    "    unk2_count.extend(g[g['s2_num_unk'] >=1]['s2_num_unk'].values.tolist())\n",
    "\n",
    "_t_num = len(unk1_token_sentence) + len(unk2_token_sentence)\n",
    "print(f\"<UNK> 토큰을 포함하고 있는 문장의 전체 개수는 {_t_num} 개 입니다.\")\n",
    "\n",
    "max_len = max(len(unk1_token_sentence), len(unk2_token_sentence))\n",
    "unk1_token_sentence += [''] * (max_len - len(unk1_token_sentence))\n",
    "unk1_count += [''] * (max_len - len(unk1_count))\n",
    "unk2_token_sentence += [''] * (max_len - len(unk2_token_sentence))\n",
    "unk2_count += [''] * (max_len - len(unk2_count))\n",
    "\n",
    "unk_pd = pd.DataFrame({'unk1_sentences': unk1_token_sentence, \n",
    "                       'unk1_count': unk1_count,\n",
    "                       'unk2_sentences': unk2_token_sentence,\n",
    "                       'unk2_count': unk2_count,\n",
    "                       })\n",
    "unk_pd.to_csv('./unk_setences_train.csv')\n",
    "print(f\"해당 문장들을 csv파일로 저장하였습니다. 파일 이름: unk_setences_train.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 어떤 단어가 UNK로 인식되었는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unk 로 인식된 토큰들 : [['오마이가뜨지져스크롸이스트휏'], ['봣는데'], ['봣는데'], ['퀼리티를'], ['됫는데'], ['줸쟝'], ['스타뎀의'], ['갖췃다'], ['스타뎀이나'], ['바르뎀의'], ['됏엇지만'], ['안봣다ㅡㅡ'], ['콱'], ['펭귄이'], ['멎진'], ['끅끅끅'], ['봣습니다'], ['꺅'], ['꿎꿎하게'], ['만드셧어'], ['사랑이뤄지길바럤는데'], ['괸찮다'], ['봣다'], ['맬'], ['믕지가'], ['훠어얼씬'], ['웩'], ['구가의서가훨씬낫다ㅉ'], ['두번봣는데'], ['퀵을'], ['번쨰에서'], ['티비로봣는데잼서요'], ['어딨으랴'], ['괞찮았다'], ['앳킨스'], ['수고하셧습니다'], ['ㅑㅋ', 'ㅑ', 'ㅑ'], ['뽈노'], ['영화네욥'], ['영호ㅓ'], ['반담횽'], ['겜'], ['편집됬다는게', '빂츠갔더니', '뙇'], ['뗀적이'], ['집중잘안됏던'], ['멋이쑈어여'], ['펭귄맨'], ['앜ㅋㅋ웃는거야', '우는거얔ㅋㅋㅋ'], ['매젬게'], ['사람이저렇게홱홱변할수가있나어처구니없다'], ['헿'], ['힛걸에의한', '힛걸을위한', '힛걸에'], ['ㅉㅉ'], ['ㅤㅅㅞㅅ'], ['검색해봤는뎁ㅋㅋ'], ['홧팅'], ['숀'], ['소꿉놀이하는'], ['되고싶어욯ㅎㅎ'], ['샹ㅂ나ㅓ'], ['틀어주는거봣는대'], ['왤케'], ['마지막횐줄모르고봣는데'], ['쑈를', '쑈', '쑈', '쑈'], ['ㅉㅉ볼줄모르면'], ['ㅉㅉ'], ['맞죵'], ['찝찝하고'], ['왤케'], ['좠는데'], ['액숀'], ['반감됬다'], ['꼬꼬마들ㅉㅉ'], ['ㅉㅉㄱ'], ['얍'], ['완전완줜'], ['됬냐'], ['조니뎁'], ['로완앳킨슨'], ['화씨과함꼐'], ['봣는데'], ['뿅'], ['호호홐'], ['왤캐'], ['ㅉㅉ'], ['웤을'], ['무조껀'], ['제ㅔㅔ발'], ['역시일본것칻더니일본거내'], ['어릴떄'], ['잼있었늣데'], ['퀵'], ['빕니다'], ['떄는'], ['쎴다'], ['ㅓㅋㅋㅋ'], ['괜탆은'], ['괸찬으나'], ['명껀'], ['잼게봤는데평정이오ㅐ이러지ㅋㅋ'], ['앍'], ['빂이어서'], ['이영화진ㄴ짜ㄱㅐ같음'], ['겜블러'], ['뙇'], ['웃었넼ㅋ'], ['재밌습니다하하핳히힣'], ['솓는'], ['영찝찝하고'], ['히히힛히힛히히히힛힛'], ['봣습니다'], ['시킵시다'], ['쵯한의'], ['무기지멱'], ['외상쎈터예산좀늘려주세요'], ['뭍게'], ['주셧으면합니다'], ['대튱령각하'], ['쑈'], ['쳬계적으로'], ['아픕니다'], ['뎃글'], ['안지킵니다'], ['명예홰손'], ['햐주세요'], ['등굣길'], ['한국을지킵시다'], ['빕니다'], ['국밈혈세'], ['정치쑈'], ['챠량'], ['일으킵시다'], ['괴씸합니다'], ['세웁시다'], ['시킵시다'], ['푀행'], ['파면해야됍니다'], ['큽니다'], ['비젼을'], ['괸하여'], ['마세요ᆢᆢ'], ['챠량'], ['포함시킵시다'], ['딸괸'], ['뜹니다'], ['뵌'], ['넵'], ['퐈이야'], ['뭔데욬ㅋㅋ'], ['진ㅉ'], ['쭈뼛쭈뼛'], ['뵐'], ['솨장님은', '아디돠스'], ['식습관입니돠아'], ['됬어요'], ['네넵'], ['보샸나요'], ['귀염뽀쨕해보이네요'], ['자윱니다'], ['꺄아'], ['쵝오네요'], ['봬요'], ['넵'], ['즁'], ['뵌다고'], ['느낍니다'], ['만나뵀습니다'], ['네네넵'], ['앖네여'], ['믓찌게'], ['커피챗'], ['꺄아'], ['네넵'], ['진촤', '아늼니꽈'], ['넵넵'], ['슬펐습니다'], ['뵐텐데'], ['넵'], ['디쟌팀'], ['킁킁킁'], ['넵넵'], ['껐다'], ['뵐게요'], ['ㅋㅋㅋ적당해보여욥'], ['이뿨요오'], ['서계시짘ㅋ'], ['옙'], ['킁'], ['왕겜'], ['봬용'], ['큽니다'], ['봬서'], ['ㅋㅋ앜ㅋㅋㅋ참새'], ['햐'], ['소듕해보여요'], ['넵'], ['넵'], ['꺄핫'], ['뵌김에'], ['넵'], ['함꼐하기로'], ['봬요'], ['좋습니닼ㅋㅋ'], ['넵'], ['챗봇에'], ['뵌'], ['뵌'], ['시킵시다'], ['가게마씸'], ['스케쥴이'], ['역싀'], ['뽜팅하세요'], ['왘ㅋㅋ'], ['넵'], ['뵐'], ['챕터'], ['봬요'], ['대죤맛입니다'], ['뵐'], ['바람쐬고'], ['아픕니다'], ['엊그저께'], ['설겆이'], ['유튭에'], ['있는건가욬ㅋ'], ['커피챗으로'], ['우왘ㅋㅋㅋ저'], ['고고해봅시닼ㅋㅋ'], ['스쾃'], ['뵐'], ['봬요'], ['봬요'], ['유튭라도'], ['즤집'], ['뼛속까지'], ['앜ㅋㅋ', '홓ㅋㅋㅋ'], ['멥'], ['앜ㅋㅋ'], ['심멎'], ['넵'], ['또봬요'], ['닉넴은'], ['꺄아'], ['꺄아'], ['있을떄'], ['티챗'], ['웃겨욬ㅋㅋ'], ['즐거웠어옄ㅋㅋ'], ['쐬러가요'], ['믱'], ['스웻셔츠를'], ['예쁩니다'], ['쐬고'], ['싶어숴'], ['낑낑대는거'], ['진심이군욬ㅋㅋ'], ['봬요'], ['바뀝니다'], ['스타뎀의'], ['스타뎀이나'], ['바르뎀의'], ['펭귄이'], ['완젼'], ['오지랖에'], ['아픕니다'], ['소소한재미ㅜㅋ여탯껏삶을다시돌아보게하는영화ㅜ'], ['짦다는게'], ['｀'], ['잴'], ['버젼으로'], ['쩝'], ['쵝오네요'], ['껐다'], ['무서웟어'], ['뵛지만'], ['수고하셧습니다'], ['뀰잼이다뀰잼이라구여'], ['ㅊ최고의'], ['미쉘누님의'], ['쯧쯧'], ['미쉘파이퍼'], ['아까웟다고'], ['울뻔햇다힝'], ['안나온닼ㅋㅋ'], ['재밋게봣어용'], ['힛걸에'], ['보셩'], ['흐믓하네요'], ['슬펐다'], ['재미잇ㄱㅔ본영화에용ㅎ'], ['됬는데'], ['아쉬웟지만'], ['이게영홥니까'], ['시간가는줄모르고봣네요'], ['왤케이쁨'], ['괸찮은데'], ['껐다'], ['찝찝하고'], ['눙물'], ['혓소리'], ['뭍어나는'], ['재밌게봣던영화'], ['전개됬다'], ['힝'], ['ㅉㅉㅉ'], ['찝찝한'], ['재미없다재미없어ㅉㅉ'], ['평점낯추기성공함ㅅㄳㄱ'], ['로보캅'], ['어렸을떄보고'], ['됫을까'], ['오빠들더멋잇어졋어요ㅎㅎ'], ['미스테잌'], ['쳇바퀴속에서'], ['헐춋류훃', 'ㅍ'], ['쵝오'], ['잼쎠요'], ['빕니다'], ['결말은찝찝하다'], ['봤늡데'], ['영화밨네요'], ['벴어요'], ['봣는데'], ['쩝'], ['스타뎀'], ['형사뭏'], ['앜ㅋㅋ'], ['땈'], ['퉷'], ['이쁘넝ㅜㅜ'], ['기대하고봣더니'], ['왤케'], ['스타뎀'], ['뿜엇닼ㅋ'], ['아하하하핳핳하하핳핳핳'], ['재미잇죵'], ['봣네'], ['☼문재인정부는'], ['지킵시다'], ['소년법폐지해주ㅛㅔ요'], ['위햐'], ['어쩝니까'], ['시킵시다'], ['내려주셧으면'], ['포함시킵시다'], ['낟낟히'], ['👌👌👌👌'], ['넵'], ['설렜다'], ['쐬고'], ['빕니다'], ['느낍니다'], ['넵'], ['빕니다'], ['넵'], ['넵', '넵'], ['규숑'], ['기쁩니다'], ['껐다가'], ['켤레를'], ['기쁩니다'], ['17켤레만'], ['왕젬'], ['꺄오오올'], ['뵐'], ['갬성에'], ['만나뵌'], ['우쌰우쌰'], ['ㅋㅋ넵'], ['꺄', '훅'], ['틀어놓나욬ㅋㅋ'], ['사진입니꽈'], ['넵'], ['넵'], ['감샤합니댱'], ['놋북'], ['빕니다'], ['넵'], ['쉑쉑버거'], ['하시죵'], ['엊그제'], ['훠궈'], ['고양이돜ㅋㅋ'], ['어딨더라'], ['후웁하게'], ['뵌것'], ['뵐게여'], ['요롷게', '요롷게'], ['어꺠춤을'], ['넵'], ['넵'], ['홧팅하세요'], ['앜ㅋㅋ'], ['일합싀다아아'], ['힣'], ['즐거웟습니다'], ['훅팀의'], ['꺄아아아'], ['뚜렷해ㅛㅓ'], ['만나뵐'], ['뺄'], ['봬요'], ['넵'], ['쫑끗하네요', '쏘큩'], ['뵐'], ['꺄아'], ['넵은'], ['네넵'], ['ㅋㅋ유튭에'], ['뵐'], ['깃헙에'], ['커피챗놓쳤네요'], ['넵'], ['봬요'], ['뵐', '봬요'], ['해봅시닼ㅋㅋ'], ['밀푀유나베'], ['유튭중'], ['챕터를'], ['힣'], ['가는군뇽'], ['굠'], ['진챠', '시작함돠'], ['뵐'], ['뵐수'], ['뵐게용'], ['넵넵'], ['아닠ㅋㅋ', '대장이에옄ㅋㅋㅋ'], ['로우라이즠ㅋㅋㅋ', '문제네옄ㅋㅋ'], ['좍좍'], ['듄을', '듄'], ['넵'], ['넵넵'], ['너무귀엽닼ㅋㅋ큐ㅠ'], ['떄려주세요'], ['넵'], ['핳'], ['봬요'], ['귀엽군욬ㅋㅋ'], ['퀵'], ['콧바람쐬러'], ['앜ㅋㅋ아쉽게도'], ['ㅈㅔ가', '으챠'], ['뵌'], ['군뇽'], ['쐬고'], ['만나뵐게요'], ['퀵'], ['갭차이'], ['재밌었습니닼ㅋㅋ'], ['시작해보시죵'], ['넵'], ['앜ㅋㅋ포즈', '저거였군욬ㅋㅋ']]\n",
      "unk로 인식된 토큰의 총 개수는 478 개 입니다.\n",
      "unk 로 인식된 토큰 리스트를 저장하였습니다. 파일 이름: unk_tokens_train.csv\n"
     ]
    }
   ],
   "source": [
    "# train data에서 어떤 단어가 <UNK>로 인식되는 것인지 확인해보자.\n",
    "\n",
    "# 재밌는 것: encode = tokenizer(text) 의 결과는 transformer에 속하는 타입으로, 내장 함수가 몇 가지 있다. ex. token_to_char ...\n",
    "#print(type(encoded))\n",
    "\n",
    "# 0. unk_pd가 저장되었다고 가정.\n",
    "\n",
    "# 0-1. setting\n",
    "import copy\n",
    "# 1. 문장에 대한 반복\n",
    "\n",
    "def find_unk_tokens(ex_unk_sentences):\n",
    "    unk_tokens = []\n",
    "    count = 0\n",
    "    \n",
    "    for unk_sentence in ex_unk_sentences:\n",
    "        _unk = []\n",
    "        # 2. encode and decode\n",
    "        encoded = tokenizer(unk_sentence)                                \n",
    "        decoded = tokenizer.convert_ids_to_tokens(encoded['input_ids'])     # 각 token id를 리스트로 리턴\n",
    "\n",
    "        # 3. 몇 번째가 unk인지 확인. 인덱스 모두 저장\n",
    "        unk_indexes = [i for i, token in enumerate(decoded) if token == tokenizer.unk_token]\n",
    "        count += len(unk_indexes)\n",
    "\n",
    "        # 4. input에서 어떤 단어가 unk였는지 확인\n",
    "        for index in unk_indexes:\n",
    "            char_index = encoded.token_to_chars(index)\n",
    "            original_token = unk_sentence[char_index.start:char_index.end]  # char_index 는 CharSpan(start=15, end=19) 형태로 리턴되더랍니다... 신기!\n",
    "            \n",
    "            #print(f\"<UNK> token: {original_token}\")\n",
    "            _unk.append(original_token)\n",
    "        \n",
    "        if _unk:\n",
    "            unk_tokens.append(_unk)\n",
    "            \n",
    "    return unk_tokens, count\n",
    "\n",
    "unk1_sentences, s1_count = find_unk_tokens(unk_pd['unk1_sentences'].values.tolist())\n",
    "unk2_sentences, s2_count  = find_unk_tokens(unk_pd['unk2_sentences'].values.tolist())\n",
    "unk_tokens = copy.deepcopy(unk1_sentences)\n",
    "unk_tokens.extend(unk2_sentences)\n",
    "print(f\"unk 로 인식된 토큰들 : {unk_tokens}\\nunk로 인식된 토큰의 총 개수는 {s1_count + s2_count} 개 입니다.\")\n",
    "\n",
    "# 5. 기존 unk_pd(sentence가 저장된 csv) 에 저장\n",
    "# 5-1. 길이 맞추기\n",
    "max_len = max(len(unk1_sentences), len(unk1_sentences))\n",
    "unk2_sentences += [''] * (max_len - len(unk2_sentences))\n",
    "unk_pd['unk1_token'] = unk1_sentences\n",
    "unk_pd['unk2_token'] = unk2_sentences\n",
    "\n",
    "unk_pd = unk_pd[['unk1_sentences', 'unk1_count', 'unk1_token', 'unk2_sentences', 'unk2_count', 'unk2_token']]\n",
    "unk_pd.head(5)\n",
    "\n",
    "# # 5. csv 로 저장\n",
    "# unk_tokens_pd = pd.DataFrame({'unk_token': unk_tokens})\n",
    "# unk_tokens_pd.to_csv(\"./unk_tokens_train.csv\")\n",
    "unk_pd.to_csv(\"./unk_tokens_train.csv\")\n",
    "\n",
    "print(f\"unk 로 인식된 토큰 리스트를 저장하였습니다. 파일 이름: unk_tokens_train.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. UNK를 포함한 문장들을 spelling check 진행\n",
    "- git clone 하고,\n",
    "- pyhanspell -> issue 들어가서 바꾸라는대로 바꾸고 재설치."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "'py-hanspell'에 복제합니다...\n",
      "remote: Enumerating objects: 101, done.\u001b[K\n",
      "remote: Counting objects: 100% (20/20), done.\u001b[K\n",
      "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
      "remote: Total 101 (delta 5), reused 10 (delta 3), pack-reused 81\u001b[K\n",
      "오브젝트를 받는 중: 100% (101/101), 25.27 KiB | 8.42 MiB/s, 완료.\n",
      "델타를 알아내는 중: 100% (42/42), 완료.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ssut/py-hanspell.git\n",
    "\n",
    "# 이후 https://github.com/ssut/py-hanspell/issues/31 에 따라 파일 내용 수정 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ilewis/git_dir/level1_semantictextsimilarity-nlp-14/JH/data_analysis/py-hanspell\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/ilewis/opt/anaconda3/lib/python3.9/site-packages/setuptools/dist.py:771: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead\n",
      "  warnings.warn(\n",
      "running install\n",
      "/Users/ilewis/opt/anaconda3/lib/python3.9/site-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  warnings.warn(\n",
      "/Users/ilewis/opt/anaconda3/lib/python3.9/site-packages/setuptools/command/easy_install.py:144: EasyInstallDeprecationWarning: easy_install command is deprecated. Use build and pip and other standards-based tools.\n",
      "  warnings.warn(\n",
      "running bdist_egg\n",
      "running egg_info\n",
      "writing py_hanspell.egg-info/PKG-INFO\n",
      "writing dependency_links to py_hanspell.egg-info/dependency_links.txt\n",
      "writing requirements to py_hanspell.egg-info/requires.txt\n",
      "writing top-level names to py_hanspell.egg-info/top_level.txt\n",
      "reading manifest file 'py_hanspell.egg-info/SOURCES.txt'\n",
      "adding license file 'LICENSE'\n",
      "writing manifest file 'py_hanspell.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.macosx-11.1-arm64/egg\n",
      "running install_lib\n",
      "running build_py\n",
      "creating build/bdist.macosx-11.1-arm64/egg\n",
      "creating build/bdist.macosx-11.1-arm64/egg/hanspell\n",
      "copying build/lib/hanspell/constants.py -> build/bdist.macosx-11.1-arm64/egg/hanspell\n",
      "copying build/lib/hanspell/__init__.py -> build/bdist.macosx-11.1-arm64/egg/hanspell\n",
      "copying build/lib/hanspell/response.py -> build/bdist.macosx-11.1-arm64/egg/hanspell\n",
      "copying build/lib/hanspell/spell_checker.py -> build/bdist.macosx-11.1-arm64/egg/hanspell\n",
      "byte-compiling build/bdist.macosx-11.1-arm64/egg/hanspell/constants.py to constants.cpython-39.pyc\n",
      "byte-compiling build/bdist.macosx-11.1-arm64/egg/hanspell/__init__.py to __init__.cpython-39.pyc\n",
      "byte-compiling build/bdist.macosx-11.1-arm64/egg/hanspell/response.py to response.cpython-39.pyc\n",
      "byte-compiling build/bdist.macosx-11.1-arm64/egg/hanspell/spell_checker.py to spell_checker.cpython-39.pyc\n",
      "creating build/bdist.macosx-11.1-arm64/egg/EGG-INFO\n",
      "copying py_hanspell.egg-info/PKG-INFO -> build/bdist.macosx-11.1-arm64/egg/EGG-INFO\n",
      "copying py_hanspell.egg-info/SOURCES.txt -> build/bdist.macosx-11.1-arm64/egg/EGG-INFO\n",
      "copying py_hanspell.egg-info/dependency_links.txt -> build/bdist.macosx-11.1-arm64/egg/EGG-INFO\n",
      "copying py_hanspell.egg-info/requires.txt -> build/bdist.macosx-11.1-arm64/egg/EGG-INFO\n",
      "copying py_hanspell.egg-info/top_level.txt -> build/bdist.macosx-11.1-arm64/egg/EGG-INFO\n",
      "zip_safe flag not set; analyzing archive contents...\n",
      "creating 'dist/py_hanspell-1.1-py3.9.egg' and adding 'build/bdist.macosx-11.1-arm64/egg' to it\n",
      "removing 'build/bdist.macosx-11.1-arm64/egg' (and everything under it)\n",
      "Processing py_hanspell-1.1-py3.9.egg\n",
      "Removing /Users/ilewis/opt/anaconda3/lib/python3.9/site-packages/py_hanspell-1.1-py3.9.egg\n",
      "Copying py_hanspell-1.1-py3.9.egg to /Users/ilewis/opt/anaconda3/lib/python3.9/site-packages\n",
      "py-hanspell 1.1 is already the active version in easy-install.pth\n",
      "\n",
      "Installed /Users/ilewis/opt/anaconda3/lib/python3.9/site-packages/py_hanspell-1.1-py3.9.egg\n",
      "Processing dependencies for py-hanspell==1.1\n",
      "Searching for requests==2.28.1\n",
      "Best match: requests 2.28.1\n",
      "Adding requests 2.28.1 to easy-install.pth file\n",
      "\n",
      "Using /Users/ilewis/opt/anaconda3/lib/python3.9/site-packages\n",
      "Searching for certifi==2022.9.24\n",
      "Best match: certifi 2022.9.24\n",
      "Adding certifi 2022.9.24 to easy-install.pth file\n",
      "\n",
      "Using /Users/ilewis/opt/anaconda3/lib/python3.9/site-packages\n",
      "Searching for urllib3==1.26.11\n",
      "Best match: urllib3 1.26.11\n",
      "Adding urllib3 1.26.11 to easy-install.pth file\n",
      "\n",
      "Using /Users/ilewis/opt/anaconda3/lib/python3.9/site-packages\n",
      "Searching for idna==3.3\n",
      "Best match: idna 3.3\n",
      "Adding idna 3.3 to easy-install.pth file\n",
      "\n",
      "Using /Users/ilewis/opt/anaconda3/lib/python3.9/site-packages\n",
      "Searching for charset-normalizer==2.0.4\n",
      "Best match: charset-normalizer 2.0.4\n",
      "Adding charset-normalizer 2.0.4 to easy-install.pth file\n",
      "Installing normalizer script to /Users/ilewis/opt/anaconda3/bin\n",
      "\n",
      "Using /Users/ilewis/opt/anaconda3/lib/python3.9/site-packages\n",
      "Finished processing dependencies for py-hanspell==1.1\n",
      "/Users/ilewis/git_dir/level1_semantictextsimilarity-nlp-14/JH/data_analysis\n"
     ]
    }
   ],
   "source": [
    "%cd ./py-hanspell\n",
    "!python ./setup.py install\n",
    "\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 맞춤법 교정 후 복원한 문장을 옆에 붙일 예정입니다. 위 셀들로부터 이어서, unk_pd 가 불러와져있다고 가정합니다.\n",
    "\n",
    "# 0. settings\n",
    "import sys\n",
    "sys.path.append('./py-hanspell')\n",
    "from tqdm import tqdm\n",
    "from hanspell import spell_checker\n",
    "\n",
    "# 1. 스펠링 체크를 진행할 문장 세트 준비\n",
    "u1_s = unk_pd['unk1_sentences'].values.tolist()\n",
    "u2_s = unk_pd['unk2_sentences'].values.tolist()\n",
    "u2_s = [item for item in u2_s if item]\n",
    "\n",
    "# 2. 함수 선언\n",
    "def check(data):\n",
    "    changed, label = [], []\n",
    "    \n",
    "    for sentence in tqdm(data):\n",
    "        # 2-1. 스펠링 체크\n",
    "        result = spell_checker.check(sentence).as_dict()\n",
    "        checked = result['checked']\n",
    "        \n",
    "        # 2-2. 결과. 문장이 전~혀 바뀌지 않는 경우에만 unchanged라고 기록되게 함. 단순히 띄어쓰기만 들어가도 changed 로 인식됨.\n",
    "        changed.append(checked)\n",
    "        label += [\"changed\" if sentence!=checked else \"unchanged\"]\n",
    "\n",
    "    return changed, label\n",
    "\n",
    "# 3. pd 저장\n",
    "changed_1, label_1 = check(u1_s)\n",
    "changed_2, label_2 = check(u2_s)\n",
    "\n",
    "# 3-1. 길이가 서로 다른 리스트를 '열' 로 붙여넣으려고 하니까, 길이가 안 맞으면 안 되서, 임시 방편으로 넣어뒀습니다. 이거때문에 좀 오히려 복잡해져서 수정예정입니다.\n",
    "changed_2 += [''] * (len(changed_1) - len(changed_2))\n",
    "label_2 += [''] * (len(label_1) - len(label_2))\n",
    "\n",
    "unk_pd['unk1_sentences_checked'] = changed_1\n",
    "unk_pd['unk1_checked_label'] = label_1\n",
    "\n",
    "\n",
    "unk_pd['unk2_sentences_checked'] = changed_2\n",
    "unk_pd['unk2_checked_label'] = label_2\n",
    "\n",
    "unk_pd = unk_pd[['unk1_sentences', 'unk1_count', 'unk1_token', 'unk1_sentences_checked', 'unk1_checked_label', \n",
    "                 'unk2_sentences', 'unk2_count', 'unk2_token', 'unk2_sentences_checked', 'unk2_checked_label']]\n",
    "\n",
    "unk_pd.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
