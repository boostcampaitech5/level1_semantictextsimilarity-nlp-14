{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNK token을 포함한 문장에는 무엇이 있는가 확인하는 ipynb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 기초 함수 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unk sentence to csv\n",
    "\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "train_path = './data/train.csv'\n",
    "dev_path = './data/dev.csv'\n",
    "\n",
    "train_data = pd.read_csv(train_path)\n",
    "dev_data = pd.read_csv(dev_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('klue/roberta-small')\n",
    "\n",
    "# Function Settings\n",
    "def get_num_tokens(df):\n",
    "    sentence1_len, sentence2_len = [], []\n",
    "    sentence1_unk, sentence2_unk = [], []\n",
    "    \n",
    "    for i, item in df.iterrows():\n",
    "        sentence1 = tokenizer(item['sentence_1'])['input_ids']\n",
    "        sentence2 = tokenizer(item['sentence_2'])['input_ids']\n",
    "\n",
    "        sentence1_len.append(len(sentence1))\n",
    "        sentence2_len.append(len(sentence2))\n",
    "\n",
    "        sentence1_unk.append(sentence1.count(tokenizer.unk_token_id))\n",
    "        sentence2_unk.append(sentence2.count(tokenizer.unk_token_id))\n",
    "\n",
    "    return sentence1_len, sentence2_len, sentence1_unk, sentence2_unk\n",
    "    # return pd.DataFrame({'number of tokens':sentence1_len, 'label score':df.label.values.tolist()})\n",
    "\n",
    "\n",
    "# 1. 전체 df에 대해 score 5단계로 분류, 열 추가\n",
    "train_data_scored = train_data.copy(deep=True)\n",
    "score_integer = []\n",
    "\n",
    "for i, item in train_data_scored.iterrows():\n",
    "    label_value = int(item['label'])\n",
    "    if   label_value == 0:  col = 0\n",
    "    elif label_value < 2.0: col = 1\n",
    "    elif label_value < 3.0: col = 2\n",
    "    elif label_value < 4.0: col = 3\n",
    "    elif label_value < 5.0: col = 4\n",
    "    else:                   col = 5\n",
    "        \n",
    "    score_integer.append(col)\n",
    "train_data_scored['score_class'] = score_integer\n",
    "\n",
    "# 2. sentence 별 토큰 개수 넣기\n",
    "s1_len, s2_len, s1_unk, s2_unk = get_num_tokens(train_data_scored)\n",
    "\n",
    "train_data_scored['s1_num_tokens'] = s1_len\n",
    "train_data_scored['s2_num_tokens'] = s2_len\n",
    "train_data_scored['s1_num_unk'] = s1_unk\n",
    "train_data_scored['s2_num_unk'] = s2_unk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. unk token 문장 확인 후 CSV 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unk sentence to csv\n",
    "\n",
    "unk1_token_sentence = []\n",
    "unk2_token_sentence = []\n",
    "unk1_count, unk2_count = [], []\n",
    "\n",
    "source_list = sorted(train_data['source'].unique())\n",
    "\n",
    "for source_name in source_list:\n",
    "    g = train_data_scored.groupby(['source']).get_group('nsmc-sampled')\n",
    "    u1_sentence = g[g['s1_num_unk'] >=1]['sentence_1'].values.tolist()\n",
    "    u2_sentence = g[g['s2_num_unk'] >=1]['sentence_2'].values.tolist()\n",
    "\n",
    "    unk1_token_sentence.extend(u1_sentence)\n",
    "    unk2_token_sentence.extend(u2_sentence)\n",
    "    unk1_count.extend(g[g['s1_num_unk'] >=1]['s1_num_unk'].values.tolist())\n",
    "    unk2_count.extend(g[g['s2_num_unk'] >=1]['s2_num_unk'].values.tolist())\n",
    "    \n",
    "\n",
    "max_len = max(len(unk1_token_sentence), len(unk2_token_sentence))\n",
    "unk1_token_sentence += [''] * (max_len - len(unk1_token_sentence))\n",
    "unk1_count += [''] * (max_len - len(unk1_count))\n",
    "unk2_token_sentence += [''] * (max_len - len(unk2_token_sentence))\n",
    "unk2_count += [''] * (max_len - len(unk2_count))\n",
    "\n",
    "unk_pd = pd.DataFrame({'unk1_sentences': unk1_token_sentence, \n",
    "                       'unk1_count': unk1_count,\n",
    "                       'unk2_sentences': unk2_token_sentence,\n",
    "                       'unk2_count': unk2_count,\n",
    "                       })\n",
    "unk_pd.to_csv('./unk_setences_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
