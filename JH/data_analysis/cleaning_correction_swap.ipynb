{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Notice\n",
    "- 주피터랩 base 환경에서 테스트해보지 않았음(설치가 되는지)\n",
    "- 주피터노트북으로 본인은 실행하였음."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install git+https://github.com/jungin500/py-hanspell\n",
    "!pip install git+https://github.com/haven-jeon/PyKoSpacing.git\n",
    "!pip install soynlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 형태소 분석을 위한 Mecab 설치\n",
    "# !python3 -m pip install --upgrade pip\n",
    "# !python3 -m pip install konlpy\n",
    "\n",
    "# ! apt-get install -y build-essential openjdk-8-jdk python3-dev curl git automake\n",
    "# ! pip install konlpy \"tweepy<4.0.0\"\n",
    "# ! pip install JPype1-py3 mecab-ko mecab-ko-dic mecab-python\n",
    "# ! /bin/bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from collections import Counter, OrderedDict\n",
    "from hanspell import spell_checker\n",
    "from pykospacing import Spacing\n",
    "\n",
    "# setting\n",
    "tokenizer = AutoTokenizer.from_pretrained('klue/roberta-small')\n",
    "spacing = Spacing()\n",
    "\n",
    "\n",
    "train_path = './data/train.csv'\n",
    "dev_path = './data/dev.csv'\n",
    "test_path = './data/test.csv'\n",
    "\n",
    "train_data = pd.read_csv(train_path)\n",
    "dev_data = pd.read_csv(dev_path)\n",
    "test_data = pd.read_csv(test_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting\n",
    "tokenizer = AutoTokenizer.from_pretrained('klue/roberta-small')\n",
    "\n",
    "train_path = './data/train.csv'\n",
    "dev_path = './data/dev.csv'\n",
    "test_path = './data/test.csv'\n",
    "\n",
    "train_data = pd.read_csv(train_path)\n",
    "dev_data = pd.read_csv(dev_path)\n",
    "test_data = pd.read_csv(test_path)\n",
    "\n",
    "#################################################################################### 함수 세팅\n",
    "def find_unk_check(sentence):\n",
    "    _encode = tokenizer(sentence)\n",
    "    unk_tokens = []\n",
    "    \n",
    "    # 1. sentence 내에 unk가 있는가?\n",
    "    if _encode['input_ids'].count(tokenizer.unk_token_id):\n",
    "        unk_tok = []\n",
    "        \n",
    "        _decode = tokenizer.convert_ids_to_tokens(_encode['input_ids'])\n",
    "        unk_idx = [i for i, token in enumerate(_decode) if token == tokenizer.unk_token]\n",
    "        \n",
    "        for _idx in unk_idx:\n",
    "            char_index = _encode.token_to_chars(_idx)\n",
    "            original_token = sentence[char_index.start:char_index.end]  # char_index 는 CharSpan(start=15, end=19) 형태로 리턴되더랍니다... 신기!\n",
    "            \n",
    "            unk_tok.append(original_token)\n",
    "        \n",
    "        # 1-1. unk token 보관 -> 나중에 이게 있는지 없는지 체크 해야함.\n",
    "        if unk_tok:\n",
    "            unk_tokens.append(unk_tok)\n",
    "            \n",
    "        # 2. spelling correction. unk있는 문장은 spelling check\n",
    "        result = spell_checker.check(sentence).as_dict()\n",
    "        checked = result['checked']\n",
    "        \n",
    "        # 3. 변화가 있었는지 체크. 변화가 없는 단어만 따로 놓기.\n",
    "        still_unk = []\n",
    "        for ut in unk_tok:\n",
    "            if checked.find(ut) != -1:\n",
    "                still_unk.append(ut)\n",
    "        \n",
    "        # 3-1. 변화가 없다면 pykospacing을 적용해보고, 다시 spelling check를 진행한다.\n",
    "        if still_unk:\n",
    "            space_checked = spacing(checked)\n",
    "            # 앜ㅋㅋ 과 같은 감정표현 정제 기능 추가\n",
    "            emo_checked = emoticon_normalize(space_checked, num_repeats=2) # ex. 안됔ㅋ큐ㅠ -> 안돼ㅋㅋ ㅜ\n",
    "            result = spell_checker.check(emo_checked).as_dict()\n",
    "            checked = result['checked']\n",
    "            \n",
    "        # 3-2. 변화가 있다면 변화된 문장으로 고쳐넣고. 그래도 변화 안 되는 문장은 내버려둔다.\n",
    "        checked_sentence = checked\n",
    "\n",
    "        # 4. unk가 인식되는지 확인.\n",
    "        _encode = tokenizer(checked_sentence)\n",
    "        space_unk = []\n",
    "\n",
    "        if _encode['input_ids'].count(tokenizer.unk_token_id):\n",
    "            _decode = tokenizer.convert_ids_to_tokens(_encode['input_ids'])\n",
    "            unk_idx = [i for i, token in enumerate(_decode) if token == tokenizer.unk_token]\n",
    "            \n",
    "            for _idx in unk_idx:\n",
    "                char_index = _encode.token_to_chars(_idx)\n",
    "                original_token = checked_sentence[char_index.start:char_index.end]  # char_index 는 CharSpan(start=15, end=19) 형태로 리턴되더랍니다... 신기!\n",
    "                space_unk.append(original_token)\n",
    "\n",
    "    else:\n",
    "        checked_sentence = sentence\n",
    "        unk_tokens, space_unk = None, None\n",
    "\n",
    "    return checked_sentence, unk_tokens, space_unk\n",
    "\n",
    "#################################################################################### 함수 세팅\n",
    "# re 라이브러리 사용해서 제거 및 교체\n",
    "def cleaning(sentence):\n",
    "    cleaned_sentence = sentence\n",
    "\n",
    "    good_pattern = r\"[ㅋㅎ]+\"\n",
    "    bad_pattern = r\"[ㅉ]+\"\n",
    "\n",
    "    space_pattern = r\"\\s+\"                        # 교체\n",
    "\n",
    "    # punctuation cleaning\n",
    "    # cleaned_sentence = re.sub(end_pattern, \".\", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\"[?]+\", \"?\", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\"[.,!;…‥~]+\", \".\", cleaned_sentence)\n",
    "    \n",
    "    # ㅋㅎㅉㅊㅠㅜ 때문에 맞춤법이 손상되는 경우 \n",
    "    if re.search(r\"[ㅋㅎㅉㅊㅠㅜ큐쿠]+\", cleaned_sentence):\n",
    "        cleaned_sentence = re.sub(r\"[ㅋㅎ]+\", \"ㅋㅋ \", cleaned_sentence)  # ㅋ이 붙어있는 글자 받침에 있던없던 바로 지워버려서, 뒷 글자는 보통 없으므로 띄워준다.\n",
    "        #cleaned_sentence = re.sub(r\"[ㅉ]+\", \"\", cleaned_sentence)      # ㅉ 은 UNK로 인식된다.\n",
    "        cleaned_sentence = re.sub(r\"[ㅊ]+\", \"ㅊ \", cleaned_sentence)\n",
    "        cleaned_sentence = re.sub(r\"[ㅠㅜ]+\", \"ㅜ \", cleaned_sentence)\n",
    "        cleaned_sentence = re.sub(r\"[큐쿠ㅉㅡ]+\", \"\", cleaned_sentence)\n",
    "\n",
    "    # UNK 통계에 따른 임의적인 sub 기능 추가\n",
    "    cleaned_sentence = re.sub(r\"[(네넵)|(네네넵)|(넵넵)|(넵)]\", \"네\", cleaned_sentence)\n",
    "    \n",
    "    # 높임표현 낮추기 기능 추가\n",
    "    cleaned_sentence = re.sub(r\"[(뵌)|(봰)]\", \"본\", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\"[(봴)|(뵐)]\", \"볼\", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\"[(봬었)|(뵈었)|(뵀)|(뵜)]\", \"봤\", cleaned_sentence)\n",
    "\n",
    "    # 기본형인 '봬' '뵈'는 어미가 달라져서 임의로 교체하기 힘들다.\n",
    "    cleaned_sentence = re.sub(r\"[(봬요)|(뵐게요)]\", \"봐요\", cleaned_sentence)\n",
    "    \n",
    "    \n",
    "    cleaned_sentence = re.sub(r\"[.]+\", \".\", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\"[:_/\\｀☼^+<>@&$#*()]+\", \" \", cleaned_sentence)\n",
    "    \n",
    "    # emoji cleaning\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    cleaned_sentence = emoji_pattern.sub(r'', cleaned_sentence)\n",
    "\n",
    "    # strip\n",
    "    cleaned_sentence = re.sub(space_pattern, \" \", cleaned_sentence)\n",
    "    cleaned_sentence = cleaned_sentence.strip()\n",
    "\n",
    "    # 문장 내에 스페이스가 없으면 띄어쓰기 해주기.\n",
    "    if cleaned_sentence.find(\" \") == -1:\n",
    "        result = spell_checker.check(cleaned_sentence).as_dict()\n",
    "        cleaned_sentence = result['checked']\n",
    "\n",
    "    return cleaned_sentence\n",
    "\n",
    "#################################################################################### 함수 세팅\n",
    "# swapping 하는 함수\n",
    "def data_swap(df, name):\n",
    "    # org df\n",
    "    df_copied = df.copy(deep=True)\n",
    "    \n",
    "    # swap df\n",
    "    df_swap = df.copy(deep=True)\n",
    "    if name.find('train') != -1 or name.find('dev') != -1:\n",
    "        df_swap = df_swap[['id', 'source', 'sentence_2', 'sentence_1', 'label', 'binary-label']]\n",
    "    else:\n",
    "        df_swap = df_swap[['id', 'source', 'sentence_2', 'sentence_1']]\n",
    "    df_swap = df_swap.rename(columns={'sentence_1': 'sentence_2', 'sentence_2': 'sentence_1'})\n",
    "\n",
    "    concated_df = pd.concat([df_copied, df_swap])\n",
    "    return concated_df\n",
    "\n",
    "\n",
    "#################################################################################### 함수 세팅\n",
    "def data_cleaning(df, _name):                    # cleaning + correction + swap + save\n",
    "    df_copied = df.copy(deep=True)\n",
    "\n",
    "    unk_tokens, still_unk_tokens = [], []\n",
    "    \n",
    "    for i, row in tqdm(df_copied.iterrows()):\n",
    "        org_sent_1, org_sent_2 = row['sentence_1'], row['sentence_2']\n",
    "        \n",
    "        # punctuation cleaning\n",
    "        punc_cleaned_sent_1 = cleaning(org_sent_1)\n",
    "        punc_cleaned_sent_2 = cleaning(org_sent_2)\n",
    "        \n",
    "        # Find UNK - spelling correction - Spacing - spelling correction again\n",
    "        checked_sent_1, _unk_tokens_1, _still_unk_tokens_1 = find_unk_check(punc_cleaned_sent_1)    # _unk_tokens_1 = 원래 unk로 인식되는 단어, \n",
    "        checked_sent_2, _unk_tokens_2, _still_unk_tokens_2 = find_unk_check(punc_cleaned_sent_2)    # still... = 스펠링 교정, spacing 한 뒤 다시 교정 과정을 거친 뒤에도 unk로 인식되는 단어.\n",
    "        \n",
    "        if _unk_tokens_1 is not None:           unk_tokens.extend(_unk_tokens_1)\n",
    "        if _unk_tokens_2 is not None:           unk_tokens.extend(_unk_tokens_2)\n",
    "        if _still_unk_tokens_1 is not None:     still_unk_tokens.extend(_still_unk_tokens_1)\n",
    "        if _still_unk_tokens_2 is not None:     still_unk_tokens.extend(_still_unk_tokens_2)\n",
    "        \n",
    "        # 원래 문장과 다르면 저장\n",
    "        if org_sent_1 != checked_sent_1:\n",
    "            df_copied.loc[i, 'sentence_1'] = checked_sent_1\n",
    "        \n",
    "        if org_sent_2 != checked_sent_2:\n",
    "            df_copied.loc[i, 'sentence_2'] = checked_sent_2\n",
    "\n",
    "    # save setting   \n",
    "    if _name.find('train') != -1:                                 name = 'train'\n",
    "    elif _name.find('dev') != -1 or _name.find('valid') != -1:    name = 'dev'\n",
    "    elif _name.find('test')!= -1:                                 name = 'test'\n",
    "    \n",
    "    # swap data(데이터 증강)\n",
    "    concated_df = data_swap(df_copied, name)\n",
    "    \n",
    "    # 혹시나 생기는 NaN 제거\n",
    "    concated_df = concated_df.replace('', np.nan, regex=True)\n",
    "    result = concated_df.dropna(axis=0, subset=['sentence_1', 'sentence_2'], inplace=False)\n",
    "\n",
    "    result.to_csv(f\"{name}_clean_correct_swap.csv\", index=False)\n",
    "    #print(f\"Spelling corrected {name} example 5 rows: \\n\", df_copied.head(5))\n",
    "    print(f\"{name} has still unk token: \\n{still_unk_tokens}\\n\\n\")\n",
    "    return still_unk_tokens\n",
    "\n",
    "# still_train_unk = data_cleaning(train_data, \"train\")\n",
    "# still_dev_unk = data_cleaning(dev_data, \"dev\")\n",
    "# still_test_unk = data_cleaning(test_data, \"test\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "still_train_unk = data_cleaning(train_data, \"train\")\n",
    "still_dev_unk = data_cleaning(dev_data, \"dev\")\n",
    "still_test_unk = data_cleaning(test_data, \"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
