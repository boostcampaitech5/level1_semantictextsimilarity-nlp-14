{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c79dae5e-66b8-4456-86a0-f3c98d1068ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, Callback\n",
    "\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "from optuna.visualization import (plot_intermediate_values, \n",
    "                                  plot_optimization_history,\n",
    "                                  plot_param_importances)\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87e08193-9d8d-497e-88af-f6634a83dd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, targets=[]):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    # 학습 및 추론 과정에서 데이터를 1개씩 꺼내오는 곳\n",
    "    def __getitem__(self, idx):\n",
    "        # 정답이 있다면 else문을, 없다면 if문을 수행합니다\n",
    "        if len(self.targets) == 0:\n",
    "            return torch.tensor(self.inputs[idx])\n",
    "        else:\n",
    "            return torch.tensor(self.inputs[idx]), torch.tensor(self.targets[idx])\n",
    "\n",
    "    # 입력하는 개수만큼 데이터를 사용합니다\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f241590e-6d64-44b5-b34c-1a8dfe4bd8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader(pl.LightningDataModule):\n",
    "    def __init__(self, model_name, batch_size, shuffle, weight_decay, train_path, dev_path, test_path, predict_path):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        self.train_path = train_path\n",
    "        self.dev_path = dev_path\n",
    "        self.test_path = test_path\n",
    "        self.predict_path = predict_path\n",
    "\n",
    "        self.train_dataset = None\n",
    "        self.val_dataset = None\n",
    "        self.test_dataset = None\n",
    "        self.predict_dataset = None\n",
    "\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, max_length=160)\n",
    "        self.target_columns = ['label']\n",
    "        self.delete_columns = ['id']\n",
    "        self.text_columns = ['sentence_1', 'sentence_2']\n",
    "\n",
    "    def tokenizing(self, dataframe):\n",
    "        data = []\n",
    "        for idx, item in tqdm(dataframe.iterrows(), desc='tokenizing', total=len(dataframe)):\n",
    "            # 두 입력 문장을 [SEP] 토큰으로 이어붙여서 전처리합니다.\n",
    "            text = '[SEP]'.join([item[text_column] for text_column in self.text_columns])\n",
    "            outputs = self.tokenizer(text, add_special_tokens=True, padding='max_length', truncation=True)\n",
    "            data.append(outputs['input_ids'])\n",
    "        return data\n",
    "\n",
    "    def preprocessing(self, data):\n",
    "        # 안쓰는 컬럼을 삭제합니다.\n",
    "        data = data.drop(columns=self.delete_columns)\n",
    "\n",
    "        # 타겟 데이터가 없으면 빈 배열을 리턴합니다.\n",
    "        try:\n",
    "            targets = data[self.target_columns].values.tolist()\n",
    "        except:\n",
    "            targets = []\n",
    "        # 텍스트 데이터를 전처리합니다.\n",
    "        inputs = self.tokenizing(data)\n",
    "\n",
    "        return inputs, targets\n",
    "\n",
    "    def setup(self, stage='fit'):\n",
    "        if stage == 'fit':\n",
    "            # 학습 데이터와 검증 데이터셋을 호출합니다\n",
    "            train_data = pd.read_csv(self.train_path)\n",
    "            val_data = pd.read_csv(self.dev_path)\n",
    "\n",
    "            # 학습데이터 준비\n",
    "            train_inputs, train_targets = self.preprocessing(train_data)\n",
    "\n",
    "            # 검증데이터 준비\n",
    "            val_inputs, val_targets = self.preprocessing(val_data)\n",
    "\n",
    "            # train 데이터만 shuffle을 적용해줍니다, 필요하다면 val, test 데이터에도 shuffle을 적용할 수 있습니다\n",
    "            self.train_dataset = Dataset(train_inputs, train_targets)\n",
    "            self.val_dataset = Dataset(val_inputs, val_targets)\n",
    "        else:\n",
    "            # 평가데이터 준비\n",
    "            test_data = pd.read_csv(self.test_path)\n",
    "            test_inputs, test_targets = self.preprocessing(test_data)\n",
    "            self.test_dataset = Dataset(test_inputs, test_targets)\n",
    "\n",
    "            predict_data = pd.read_csv(self.predict_path)\n",
    "            predict_inputs, predict_targets = self.preprocessing(predict_data)\n",
    "            self.predict_dataset = Dataset(predict_inputs, [])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=self.shuffle)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.val_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.test_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.predict_dataset, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "773501d0-7f46-4aa8-90c9-cd3cdf3f98bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(self, model_name, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.lr = lr\n",
    "\n",
    "        # 사용할 모델을 호출합니다.\n",
    "        self.plm = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "            pretrained_model_name_or_path=model_name, num_labels=1)\n",
    "        # Loss 계산을 위해 사용될 L1Loss를 호출합니다.\n",
    "        self.loss_func = torch.nn.L1Loss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.plm(x)['logits']\n",
    "\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_func(logits, y.float())\n",
    "        self.log(\"train_loss\", loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_func(logits, y.float())\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "        self.log(\"val_pearson\", torchmetrics.functional.pearson_corrcoef(logits.squeeze(), y.squeeze()))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "\n",
    "        self.log(\"test_pearson\", torchmetrics.functional.pearson_corrcoef(logits.squeeze(), y.squeeze()))\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x = batch\n",
    "        logits = self(x)\n",
    "\n",
    "        return logits.squeeze()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6a02b3df-5625-4372-ab29-60def5aa747e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # 하이퍼 파라미터 값 탐색 대상 설정\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32])\n",
    "    max_epoch = trial.suggest_int(\"max_epoch\", 1, 1)\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-5)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-2, 1e-2)\n",
    "    # gradient_clip_val = trial.suggest_float(\"gradient_clip_val\", 0, .5)\n",
    "\n",
    "    # 하이퍼 파라미터 설정\n",
    "    folder_path = '~/level1_semantictextsimilarity-nlp-14/SH'\n",
    "    args = argparse.Namespace(\n",
    "        model_name='klue/roberta-small',\n",
    "        batch_size=batch_size,\n",
    "        max_epoch=max_epoch,\n",
    "        shuffle=True,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        train_path=folder_path+'/data/train.csv',\n",
    "        dev_path=folder_path+'/data/dev.csv',\n",
    "        test_path=folder_path+'/data/dev.csv',\n",
    "        predict_path=folder_path+'/data/test.csv'\n",
    "    )\n",
    "\n",
    "    # dataloader와 model을 생성합니다.\n",
    "    dataloader = Dataloader(args.model_name, \n",
    "                            args.batch_size, \n",
    "                            args.shuffle,\n",
    "                            args.weight_decay,\n",
    "                            args.train_path, \n",
    "                            args.dev_path, \n",
    "                            args.test_path, \n",
    "                            args.predict_path)\n",
    "    model = Model(args.model_name, args.learning_rate)\n",
    "\n",
    "    # gpu가 없으면 accelerator='cpu', 있으면 accelerator='gpu'\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    accelerator = 'gpu' if device == torch.device('cuda') else 'cpu'\n",
    "    checkpoints_filename = model_name = f\"./checkpoints/{args.model_name}_bs{args.batch_size}_me{args.max_epoch}_lr{args.learning_rate}_wd{args.weight_decay}\"\n",
    "    checkpoints_path = folder_path + '/checkpoitns'\n",
    "    checkpoint_save = ModelCheckpoint(dirpath=checkpoints_path, filename=checkpoints_filename)\n",
    "    trainer = pl.Trainer(accelerator=accelerator, \n",
    "                         max_epochs=args.max_epoch, \n",
    "                         log_every_n_steps=1, \n",
    "                         # gradient_clip_val=gradient_clip_val,\n",
    "                         callbacks=[PyTorchLightningPruningCallback(trial, monitor=monitor), checkpoint_save])\n",
    "\n",
    "    # Train part\n",
    "    trainer.fit(model=model, datamodule=dataloader)\n",
    "    trainer.test(model=model, datamodule=dataloader)\n",
    "    \n",
    "    metric = trainer.callback_metrics['val_loss'].item()\n",
    "    \n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d2792ee2-f502-433e-aed0-0ed190ce4191",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 970514\n",
      "\u001b[32m[I 2023-04-11 16:05:54,085]\u001b[0m A new study created in memory with name: no-name-ddfb373b-f63d-4411-bf57-415167fe89bd\u001b[0m\n",
      "/tmp/ipykernel_2622/2666820931.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-5)\n",
      "Some weights of the model checkpoint at klue/roberta-small were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-small and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "tokenizing:   0%|          | 0/9324 [00:00<?, ?it/s]\u001b[A\n",
      "tokenizing:   3%|▎         | 293/9324 [00:00<00:03, 2925.13it/s]\u001b[A\n",
      "tokenizing:   6%|▋         | 586/9324 [00:00<00:02, 2922.59it/s]\u001b[A\n",
      "tokenizing:   9%|▉         | 879/9324 [00:00<00:02, 2919.49it/s]\u001b[A\n",
      "tokenizing:  13%|█▎        | 1178/9324 [00:00<00:02, 2945.16it/s]\u001b[A\n",
      "tokenizing:  16%|█▌        | 1482/9324 [00:00<00:02, 2978.33it/s]\u001b[A\n",
      "tokenizing:  19%|█▉        | 1780/9324 [00:00<00:02, 2974.12it/s]\u001b[A\n",
      "tokenizing:  22%|██▏       | 2078/9324 [00:00<00:02, 2973.27it/s]\u001b[A\n",
      "tokenizing:  25%|██▌       | 2376/9324 [00:00<00:02, 2974.01it/s]\u001b[A\n",
      "tokenizing:  29%|██▊       | 2675/9324 [00:00<00:02, 2977.85it/s]\u001b[A\n",
      "tokenizing:  32%|███▏      | 2973/9324 [00:01<00:02, 2957.70it/s]\u001b[A\n",
      "tokenizing:  35%|███▌      | 3275/9324 [00:01<00:02, 2976.53it/s]\u001b[A\n",
      "tokenizing:  38%|███▊      | 3573/9324 [00:01<00:01, 2947.13it/s]\u001b[A\n",
      "tokenizing:  41%|████▏     | 3869/9324 [00:01<00:01, 2949.19it/s]\u001b[A\n",
      "tokenizing:  45%|████▍     | 4168/9324 [00:01<00:01, 2957.62it/s]\u001b[A\n",
      "tokenizing:  48%|████▊     | 4464/9324 [00:01<00:01, 2958.24it/s]\u001b[A\n",
      "tokenizing:  51%|█████     | 4765/9324 [00:01<00:01, 2971.27it/s]\u001b[A\n",
      "tokenizing:  54%|█████▍    | 5063/9324 [00:01<00:01, 2957.03it/s]\u001b[A\n",
      "tokenizing:  58%|█████▊    | 5363/9324 [00:01<00:01, 2967.67it/s]\u001b[A\n",
      "tokenizing:  61%|██████    | 5660/9324 [00:01<00:01, 2959.25it/s]\u001b[A\n",
      "tokenizing:  64%|██████▍   | 5960/9324 [00:02<00:01, 2970.18it/s]\u001b[A\n",
      "tokenizing:  67%|██████▋   | 6262/9324 [00:02<00:01, 2982.01it/s]\u001b[A\n",
      "tokenizing:  70%|███████   | 6561/9324 [00:02<00:00, 2972.21it/s]\u001b[A\n",
      "tokenizing:  74%|███████▎  | 6862/9324 [00:02<00:00, 2982.31it/s]\u001b[A\n",
      "tokenizing:  77%|███████▋  | 7161/9324 [00:02<00:00, 2754.69it/s]\u001b[A\n",
      "tokenizing:  80%|████████  | 7463/9324 [00:02<00:00, 2829.22it/s]\u001b[A\n",
      "tokenizing:  83%|████████▎ | 7758/9324 [00:02<00:00, 2863.27it/s]\u001b[A\n",
      "tokenizing:  86%|████████▋ | 8056/9324 [00:02<00:00, 2897.11it/s]\u001b[A\n",
      "tokenizing:  90%|████████▉ | 8353/9324 [00:02<00:00, 2916.82it/s]\u001b[A\n",
      "tokenizing:  93%|█████████▎| 8653/9324 [00:02<00:00, 2939.97it/s]\u001b[A\n",
      "tokenizing:  96%|█████████▌| 8954/9324 [00:03<00:00, 2960.24it/s]\u001b[A\n",
      "tokenizing: 100%|██████████| 9324/9324 [00:03<00:00, 2940.77it/s]\u001b[A\n",
      "\n",
      "tokenizing:   0%|          | 0/550 [00:00<?, ?it/s]\u001b[A\n",
      "tokenizing: 100%|██████████| 550/550 [00:00<00:00, 2998.30it/s]\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                             | Params\n",
      "---------------------------------------------------------------\n",
      "0 | plm       | RobertaForSequenceClassification | 68.1 M\n",
      "1 | loss_func | L1Loss                           | 0     \n",
      "---------------------------------------------------------------\n",
      "68.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "68.1 M    Total params\n",
      "272.367   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 292/292 [02:17<00:00,  2.13it/s, v_num=24]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|▌         | 1/18 [00:00<00:00, 82.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█         | 2/18 [00:00<00:00, 79.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|█▋        | 3/18 [00:00<00:00, 78.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|██▏       | 4/18 [00:00<00:00, 24.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|██▊       | 5/18 [00:00<00:00, 16.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|███▎      | 6/18 [00:00<00:00, 13.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███▉      | 7/18 [00:00<00:00, 11.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|████▍     | 8/18 [00:00<00:00, 10.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 9/18 [00:00<00:00,  9.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████▌    | 10/18 [00:01<00:00,  9.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████    | 11/18 [00:01<00:00,  9.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████▋   | 12/18 [00:01<00:00,  8.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████▏  | 13/18 [00:01<00:00,  8.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|███████▊  | 14/18 [00:01<00:00,  8.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|████████▎ | 15/18 [00:01<00:00,  8.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████▉ | 16/18 [00:01<00:00,  8.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|█████████▍| 17/18 [00:02<00:00,  8.07it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 18/18 [00:02<00:00,  8.50it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2023-04-11 16:08:18,891]\u001b[0m Trial 0 failed with parameters: {'batch_size': 32, 'max_epoch': 1, 'learning_rate': 1e-05, 'weight_decay': 0.01} because of the following error: AttributeError(\"'Trainer' object has no attribute 'training_type_plugin'\").\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_2622/2666820931.py\", line 48, in objective\n",
      "    trainer.fit(model=model, datamodule=dataloader)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 520, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py\", line 44, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 559, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 935, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 978, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py\", line 201, in run\n",
      "    self.advance()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py\", line 354, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 134, in run\n",
      "    self.on_advance_end()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 248, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py\", line 177, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 122, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 258, in on_run_end\n",
      "    self._on_evaluation_end()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 303, in _on_evaluation_end\n",
      "    call._call_callback_hooks(trainer, hook_name, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py\", line 190, in _call_callback_hooks\n",
      "    fn(trainer, trainer.lightning_module, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/optuna/integration/pytorch_lightning.py\", line 99, in on_validation_end\n",
      "    should_stop = trainer.training_type_plugin.broadcast(should_stop)\n",
      "AttributeError: 'Trainer' object has no attribute 'training_type_plugin'\n",
      "\u001b[33m[W 2023-04-11 16:08:18,893]\u001b[0m Trial 0 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Trainer' object has no attribute 'training_type_plugin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m monitor \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 하이퍼파라미터 최적화 결과 출력\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of finished trials: \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(study\u001b[38;5;241m.\u001b[39mtrials))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/optuna/study/study.py:425\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    323\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    330\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[39], line 48\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     41\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(accelerator\u001b[38;5;241m=\u001b[39maccelerator, \n\u001b[1;32m     42\u001b[0m                      max_epochs\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mmax_epoch, \n\u001b[1;32m     43\u001b[0m                      log_every_n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \n\u001b[1;32m     44\u001b[0m                      \u001b[38;5;66;03m# gradient_clip_val=gradient_clip_val,\u001b[39;00m\n\u001b[1;32m     45\u001b[0m                      callbacks\u001b[38;5;241m=\u001b[39m[PyTorchLightningPruningCallback(trial, monitor\u001b[38;5;241m=\u001b[39mmonitor)])\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Train part\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtest(model\u001b[38;5;241m=\u001b[39mmodel, datamodule\u001b[38;5;241m=\u001b[39mdataloader)\n\u001b[1;32m     51\u001b[0m metric \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mcallback_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:520\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    518\u001b[0m model \u001b[38;5;241m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m--> 520\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:559\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector\u001b[38;5;241m.\u001b[39mattach_data(\n\u001b[1;32m    550\u001b[0m     model, train_dataloaders\u001b[38;5;241m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[38;5;241m=\u001b[39mval_dataloaders, datamodule\u001b[38;5;241m=\u001b[39mdatamodule\n\u001b[1;32m    551\u001b[0m )\n\u001b[1;32m    553\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    555\u001b[0m     ckpt_path,\n\u001b[1;32m    556\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    557\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    558\u001b[0m )\n\u001b[0;32m--> 559\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:935\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    932\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 935\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    940\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:978\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m    977\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m--> 978\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    979\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:201\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:354\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher\u001b[38;5;241m.\u001b[39msetup(combined_loader)\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 354\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/training_epoch_loop.py:134\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance(data_fetcher)\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_advance_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/training_epoch_loop.py:248\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.on_advance_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_check_val:\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mvalidating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# update plateau LR scheduler after metrics are logged\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:177\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/evaluation_loop.py:122\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_dataloader_outputs()\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_run_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/evaluation_loop.py:258\u001b[0m, in \u001b[0;36m_EvaluationLoop.on_run_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39mlog_eval_end_metrics(all_logged_outputs)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# hook\u001b[39;00m\n\u001b[0;32m--> 258\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_evaluation_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;66;03m# enable train mode again\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_evaluation_model_train()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/evaluation_loop.py:303\u001b[0m, in \u001b[0;36m_EvaluationLoop._on_evaluation_end\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[1;32m    302\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_test_end\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_validation_end\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 303\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_callback_hooks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(trainer, hook_name, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    305\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_strategy_hook(trainer, hook_name, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:190\u001b[0m, in \u001b[0;36m_call_callback_hooks\u001b[0;34m(trainer, hook_name, monitoring_callbacks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callable(fn):\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Callback]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcallback\u001b[38;5;241m.\u001b[39mstate_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 190\u001b[0m             \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pl_module:\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/optuna/integration/pytorch_lightning.py:99\u001b[0m, in \u001b[0;36mPyTorchLightningPruningCallback.on_validation_end\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trial\u001b[38;5;241m.\u001b[39mreport(current_score\u001b[38;5;241m.\u001b[39mitem(), step\u001b[38;5;241m=\u001b[39mepoch)\n\u001b[1;32m     98\u001b[0m     should_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trial\u001b[38;5;241m.\u001b[39mshould_prune()\n\u001b[0;32m---> 99\u001b[0m should_stop \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_type_plugin\u001b[49m\u001b[38;5;241m.\u001b[39mbroadcast(should_stop)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_stop:\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Trainer' object has no attribute 'training_type_plugin'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    pl.seed_everything(970514, workers=True)\n",
    "    monitor = 'val_loss'\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=3)\n",
    "\n",
    "    # 하이퍼파라미터 최적화 결과 출력\n",
    "    print('Number of finished trials: ', len(study.trials))\n",
    "    print('Best trial:')\n",
    "    trial = study.best_trial\n",
    "    print('  Value: {}'.format(trial.value))\n",
    "    print('  Params: ')\n",
    "    for key, value in trial.params.items():\n",
    "        print('    {}: {}'.format(key, value))\n",
    "    plot_optimization_history(study).show()\n",
    "    plot_intermediate_values(study).show()\n",
    "    try:\n",
    "        plot_param_importances(study).show()\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    # 최적의 하이퍼파라미터로 모델 생성\n",
    "    folder_path = '~/level1_semantictextsimilarity-nlp-14/SH'\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model_name', default='klue/roberta-small', type=str)\n",
    "    parser.add_argument('--shuffle', default=True)\n",
    "    parser.add_argument('--train_path', default=folder_path+'/data/train.csv')\n",
    "    parser.add_argument('--dev_path', default=folder_path+'/data/dev.csv')\n",
    "    parser.add_argument('--test_path', default=folder_path+'/data/dev.csv')\n",
    "    parser.add_argument('--predict_path', default=folder_path+'/data/test.csv')\n",
    "    args_parser = parser.parse_args()\n",
    "    \n",
    "    args = argparse.Namespace(\n",
    "        model_name=args_parser.model_name,\n",
    "        batch_size=trial.params['batch_size'],\n",
    "        max_epoch=trial.params['max_epoch'],\n",
    "        shuffle=args_parser.shuffle,\n",
    "        learning_rate=trial.params['learning_rate'],\n",
    "        weight_decay=trial.params['weight_decay'],\n",
    "        train_path=args_parser.train_path,\n",
    "        dev_path=args_parser.dev_path,\n",
    "        test_path=args_parser.test_path,\n",
    "        predict_path=args_parser.predict_path,\n",
    "    )\n",
    "    dataloader = Dataloader(args.model_name, args.batch_size, args.shuffle, args.weight_decay, args.train_path, args.dev_path,\n",
    "                            args.test_path, args.predict_path)\n",
    "    model = Model(args.model_name, args.learning_rate)\n",
    "    \n",
    "    trainer.fit(model=model, datamodule=dataloader)\n",
    "    \n",
    "    model_name = f\"./models/{args.model_name}_bs{args.batch_size}_me{args.max_epoch}_lr{args.learning_rate}_wd{args.weight_decay}.pt\"\n",
    "    torch.save(model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d224cc9-5552-4965-bc29-ea933a11a811",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
